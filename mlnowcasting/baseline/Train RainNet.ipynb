{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RainNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2020 Georgy Ayzel\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '../../data/datasets/example_dataset.npy'\n",
    "val_dataset = '../../data/datasets/example_dataset.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import gc\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.backend.tensorflow_backend import clear_session\n",
    "from keras.backend.tensorflow_backend import get_session\n",
    "import tensorflow\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,dataset,batch_size=4, frame=10, mode=\"train\"):\n",
    "        'Initialization'\n",
    "        self.mode = mode\n",
    "        self.frame = frame\n",
    "        self.batch_inds = []\n",
    "        self.dataset = dataset\n",
    "        self.sequences = np.load(dataset, mmap_mode='r').shape[0] # mmap_mode='r' should only load the header\n",
    "        #self.on_epoch_end()\n",
    "        if batch_size > self.sequences:\n",
    "            self.batch_size = int(np.ceil(self.sequences/2))\n",
    "            print(\"Using batch_size {} instead of {}\".format(self.batch_size,batch_size))\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(self.sequences/self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        start_index = index * self.batch_size\n",
    "        self.data = read_npy_chunk(self.dataset, start_index, self.batch_size)\n",
    "        return self.__data_generation()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        print(\"gc.collect()\",gc.collect()) # if it's done something you should see a number being outputted\n",
    "  \n",
    "    def __data_generation(self):\n",
    "        length = self.data.shape[1]\n",
    "        \n",
    "        self.encoder_input_data  = self.data[:, 0:self.frame,          :, :]/255.0\n",
    "        self.decoder_input_data  = self.data[:, self.frame-1:length-1, :, :]/255.0\n",
    "        self.decoder_output_data = self.data[:, self.frame:self.frame+1,     :, :]/255.0\n",
    "        \n",
    "        X = np.swapaxes(self.encoder_input_data, 1,3)\n",
    "        Y = np.swapaxes(self.decoder_output_data, 1,3)\n",
    "\n",
    "        self.data = []\n",
    "        return X, Y\n",
    "    \n",
    "import struct\n",
    "import numpy\n",
    "\n",
    "def read_npy_chunk(filename, start_row, num_rows):\n",
    "    assert start_row >= 0 and num_rows > 0\n",
    "    with open(filename, 'rb') as fhandle:\n",
    "        major, minor = numpy.lib.format.read_magic(fhandle)\n",
    "        shape, fortran, dtype = numpy.lib.format.read_array_header_1_0(fhandle)\n",
    "        assert not fortran, \"Fortran order arrays not supported\"\n",
    "        # Make sure the offsets aren't invalid.\n",
    "        assert start_row < shape[0], (\n",
    "            'start_row is beyond end of file'\n",
    "        )\n",
    "        assert start_row + num_rows <= shape[0], (\n",
    "            'start_row + num_rows > shape[0]'\n",
    "        )\n",
    "        # Get the number of elements in one 'row' by taking\n",
    "        # a product over all other dimensions.\n",
    "        row_size = numpy.prod(shape[1:])\n",
    "        start_byte = start_row * row_size * dtype.itemsize\n",
    "        fhandle.seek(start_byte, 1)\n",
    "        n_items = row_size * num_rows\n",
    "        flat = numpy.fromfile(fhandle, count=n_items, dtype=dtype)\n",
    "        return flat.reshape((-1,) + shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rainnet(input_shape=(64, 64, 16), mode=\"regression\"):\n",
    "\n",
    "    \"\"\"\n",
    "    The function for building the RainNet (v1.0) model from scratch\n",
    "    using Keras functional API.\n",
    "    Parameters:\n",
    "    input size: tuple(W x H x C), where W (width) and H (height)\n",
    "    describe spatial dimensions of input data (e.g., 928x928 for RY data);\n",
    "    and C (channels) describes temporal (depth) dimension of \n",
    "    input data (e.g., 4 means accounting four latest radar scans at time\n",
    "    t-15, t-10, t-5 minutes, and t)\n",
    "    \n",
    "    mode: \"regression\" (default) or \"segmentation\". \n",
    "    For \"regression\" mode the last activation function is linear, \n",
    "    while for \"segmentation\" it is sigmoid.\n",
    "    To train RainNet to predict continuous precipitation intensities use \n",
    "    \"regression\" mode. \n",
    "    RainNet could be trained to predict the exceedance of specific intensity \n",
    "    thresholds. For that purpose, use \"segmentation\" mode.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    conv1f = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1f = Activation(\"relu\")(conv1f)\n",
    "    conv1s = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(conv1f)\n",
    "    conv1s = Activation(\"relu\")(conv1s)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1s)\n",
    "\n",
    "    conv2f = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2f = Activation(\"relu\")(conv2f)\n",
    "    conv2s = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(conv2f)\n",
    "    conv2s = Activation(\"relu\")(conv2s)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2s)\n",
    "\n",
    "    conv3f = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3f = Activation(\"relu\")(conv3f)\n",
    "    conv3s = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(conv3f)\n",
    "    conv3s = Activation(\"relu\")(conv3s)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3s)\n",
    "\n",
    "    conv4f = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4f = Activation(\"relu\")(conv4f)\n",
    "    conv4s = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(conv4f)\n",
    "    conv4s = Activation(\"relu\")(conv4s)\n",
    "    drop4 = Dropout(0.5)(conv4s)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5f = Conv2D(1024, 3, padding='same', kernel_initializer='he_normal')(pool4)\n",
    "    conv5f = Activation(\"relu\")(conv5f)\n",
    "    conv5s = Conv2D(1024, 3, padding='same', kernel_initializer='he_normal')(conv5f)\n",
    "    conv5s = Activation(\"relu\")(conv5s)\n",
    "    drop5 = Dropout(0.5)(conv5s)\n",
    "\n",
    "    up6 = Concatenate([UpSampling2D(size=(2, 2))(drop5), conv4s], axis=3)\n",
    "    conv6 = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(up6)\n",
    "    conv6 = Activation(\"relu\")(conv6)\n",
    "    conv6 = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(conv6)\n",
    "    conv6 = Activation(\"relu\")(conv6)\n",
    "\n",
    "    up7 = Concatenate([UpSampling2D(size=(2, 2))(conv6), conv3s], axis=3)\n",
    "    conv7 = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(up7)\n",
    "    conv7 = Activation(\"relu\")(conv7)\n",
    "    conv7 = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(conv7)\n",
    "    conv7 = Activation(\"relu\")(conv7)\n",
    "\n",
    "    up8 = Concatenate([UpSampling2D(size=(2, 2))(conv7), conv2s], axis=3)\n",
    "    conv8 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(up8)\n",
    "    conv8 = Activation(\"relu\")(conv8)\n",
    "    conv8 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(conv8)\n",
    "    conv8 = Activation(\"relu\")(conv8)\n",
    "\n",
    "    up9 = Concatenate([UpSampling2D(size=(2, 2))(conv8), conv1s], axis=3)\n",
    "    conv9 = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(up9)\n",
    "    conv9 = Activation(\"relu\")(conv9)\n",
    "    conv9 = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    conv9 = Activation(\"relu\")(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "    \n",
    "    if mode == \"regression\":\n",
    "        outputs = Conv2D(1, 1, activation='linear')(conv9)\n",
    "    elif mode == \"segmentation\":\n",
    "        outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model and save as image a scheme of the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rainnet(input_shape=(64, 64, 16), mode=\"regression\")\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "model.compile(loss='logcosh', optimizer=opt)\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "301/301 [==============================] - 153s 509ms/step - loss: 0.0020 - val_loss: 6.5633e-04\n",
      "Epoch 2/300\n",
      "301/301 [==============================] - 151s 502ms/step - loss: 0.0011 - val_loss: 5.2194e-04\n",
      "Epoch 3/300\n",
      "301/301 [==============================] - 151s 502ms/step - loss: 9.5591e-04 - val_loss: 4.8073e-04\n",
      "Epoch 4/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 8.9663e-04 - val_loss: 4.5985e-04\n",
      "Epoch 5/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 8.5272e-04 - val_loss: 4.5145e-04\n",
      "Epoch 6/300\n",
      "301/301 [==============================] - 152s 503ms/step - loss: 8.1625e-04 - val_loss: 4.4706e-04\n",
      "Epoch 7/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 7.8603e-04 - val_loss: 4.4537e-04\n",
      "Epoch 8/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 7.5891e-04 - val_loss: 4.4582e-04\n",
      "Epoch 9/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 7.3484e-04 - val_loss: 4.4842e-04\n",
      "Epoch 10/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 7.1221e-04 - val_loss: 4.5184e-04\n",
      "Epoch 11/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 6.8774e-04 - val_loss: 4.5870e-04\n",
      "Epoch 12/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 6.7117e-04 - val_loss: 4.5767e-04\n",
      "Epoch 13/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 6.7191e-04 - val_loss: 4.5999e-04\n",
      "Epoch 14/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 6.6544e-04 - val_loss: 4.6163e-04\n",
      "Epoch 15/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 6.3453e-04 - val_loss: 4.5716e-04\n",
      "Epoch 16/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 6.2052e-04 - val_loss: 4.6806e-04\n",
      "Epoch 17/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 6.0710e-04 - val_loss: 4.6138e-04\n",
      "Epoch 18/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.9498e-04 - val_loss: 4.6574e-04\n",
      "Epoch 19/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.8584e-04 - val_loss: 4.6537e-04\n",
      "Epoch 20/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.8968e-04 - val_loss: 4.7077e-04\n",
      "Epoch 21/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 5.7338e-04 - val_loss: 4.6766e-04\n",
      "Epoch 22/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.6915e-04 - val_loss: 4.7192e-04\n",
      "Epoch 23/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.6138e-04 - val_loss: 4.6739e-04\n",
      "Epoch 24/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 5.4391e-04 - val_loss: 4.6107e-04\n",
      "Epoch 25/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 5.3444e-04 - val_loss: 4.5231e-04\n",
      "Epoch 26/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.2473e-04 - val_loss: 4.4467e-04\n",
      "Epoch 27/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 5.1415e-04 - val_loss: 4.4319e-04\n",
      "Epoch 28/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.0070e-04 - val_loss: 4.4275e-04\n",
      "Epoch 29/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 4.9020e-04 - val_loss: 4.4111e-04\n",
      "Epoch 30/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 4.8502e-04 - val_loss: 4.3840e-04\n",
      "Epoch 31/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 4.7516e-04 - val_loss: 4.3842e-04\n",
      "Epoch 32/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 4.6809e-04 - val_loss: 4.4058e-04\n",
      "Epoch 33/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 4.7676e-04 - val_loss: 4.3067e-04\n",
      "Epoch 34/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 4.7172e-04 - val_loss: 4.3430e-04\n",
      "Epoch 35/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 4.4729e-04 - val_loss: 4.3272e-04\n",
      "Epoch 36/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 4.3445e-04 - val_loss: 4.3099e-04\n",
      "Epoch 37/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 4.1793e-04 - val_loss: 4.2765e-04\n",
      "Epoch 38/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 4.0557e-04 - val_loss: 4.2882e-04\n",
      "Epoch 39/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 4.0794e-04 - val_loss: 4.2586e-04\n",
      "Epoch 40/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 3.9352e-04 - val_loss: 4.2563e-04\n",
      "Epoch 41/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 3.8911e-04 - val_loss: 4.1953e-04\n",
      "Epoch 42/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 3.7890e-04 - val_loss: 4.1908e-04\n",
      "Epoch 43/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 3.6721e-04 - val_loss: 4.1777e-04\n",
      "Epoch 44/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 3.6699e-04 - val_loss: 4.1951e-04\n",
      "Epoch 45/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 3.6255e-04 - val_loss: 4.1758e-04\n",
      "Epoch 46/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 3.6408e-04 - val_loss: 4.1696e-04\n",
      "Epoch 47/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 3.6334e-04 - val_loss: 4.1524e-04\n",
      "Epoch 48/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 3.7044e-04 - val_loss: 4.1641e-04\n",
      "Epoch 49/300\n",
      "301/301 [==============================] - 150s 497ms/step - loss: 3.6281e-04 - val_loss: 4.1084e-04\n",
      "Epoch 50/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 3.5509e-04 - val_loss: 4.1160e-04\n",
      "Epoch 51/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 3.4458e-04 - val_loss: 4.1199e-04\n",
      "Epoch 52/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 3.3427e-04 - val_loss: 4.1372e-04\n",
      "Epoch 53/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 3.3403e-04 - val_loss: 4.1382e-04\n",
      "Epoch 54/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 3.3435e-04 - val_loss: 4.1274e-04\n",
      "Epoch 55/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 3.3492e-04 - val_loss: 4.1167e-04\n",
      "Epoch 56/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 3.4267e-04 - val_loss: 4.1100e-04\n",
      "Epoch 57/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 3.3744e-04 - val_loss: 4.0982e-04\n",
      "Epoch 58/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 3.4064e-04 - val_loss: 4.0747e-04\n",
      "Epoch 59/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 3.2930e-04 - val_loss: 4.0671e-04\n",
      "Epoch 60/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 3.2095e-04 - val_loss: 4.0634e-04\n",
      "Epoch 61/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 3.1522e-04 - val_loss: 4.0765e-04\n",
      "Epoch 62/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 3.0462e-04 - val_loss: 4.0844e-04\n",
      "Epoch 63/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.9074e-04 - val_loss: 4.0890e-04\n",
      "Epoch 64/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.8317e-04 - val_loss: 4.0861e-04\n",
      "Epoch 65/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.8329e-04 - val_loss: 4.1057e-04\n",
      "Epoch 66/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.7586e-04 - val_loss: 4.0615e-04\n",
      "Epoch 67/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 2.7911e-04 - val_loss: 4.0758e-04\n",
      "Epoch 68/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.6626e-04 - val_loss: 4.0632e-04\n",
      "Epoch 69/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.6079e-04 - val_loss: 4.0549e-04\n",
      "Epoch 70/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.6750e-04 - val_loss: 4.0497e-04\n",
      "Epoch 71/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.6479e-04 - val_loss: 4.0351e-04\n",
      "Epoch 72/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 2.6998e-04 - val_loss: 4.0510e-04\n",
      "Epoch 73/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 150s 500ms/step - loss: 2.6798e-04 - val_loss: 4.0568e-04\n",
      "Epoch 74/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.6683e-04 - val_loss: 4.0456e-04\n",
      "Epoch 75/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.7045e-04 - val_loss: 4.0641e-04\n",
      "Epoch 76/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.8332e-04 - val_loss: 4.1018e-04\n",
      "Epoch 77/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.9595e-04 - val_loss: 4.0539e-04\n",
      "Epoch 78/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.7937e-04 - val_loss: 4.0787e-04\n",
      "Epoch 79/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.5687e-04 - val_loss: 4.1005e-04\n",
      "Epoch 80/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.4061e-04 - val_loss: 4.1143e-04\n",
      "Epoch 81/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.2906e-04 - val_loss: 4.1040e-04\n",
      "Epoch 82/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.2120e-04 - val_loss: 4.0917e-04\n",
      "Epoch 83/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.2074e-04 - val_loss: 4.1032e-04\n",
      "Epoch 84/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.1411e-04 - val_loss: 4.0834e-04\n",
      "Epoch 85/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.1344e-04 - val_loss: 4.0619e-04\n",
      "Epoch 86/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.1111e-04 - val_loss: 4.0336e-04\n",
      "Epoch 87/300\n",
      "301/301 [==============================] - 151s 503ms/step - loss: 2.0990e-04 - val_loss: 4.0765e-04\n",
      "Epoch 88/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.0901e-04 - val_loss: 4.0552e-04\n",
      "Epoch 89/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.0875e-04 - val_loss: 4.0424e-04\n",
      "Epoch 90/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.0521e-04 - val_loss: 4.0376e-04\n",
      "Epoch 91/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.9721e-04 - val_loss: 4.0232e-04\n",
      "Epoch 92/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.9952e-04 - val_loss: 4.0277e-04\n",
      "Epoch 93/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.0410e-04 - val_loss: 4.0592e-04\n",
      "Epoch 94/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.1174e-04 - val_loss: 4.0353e-04\n",
      "Epoch 95/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.1427e-04 - val_loss: 4.0037e-04\n",
      "Epoch 96/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.1643e-04 - val_loss: 4.0819e-04\n",
      "Epoch 97/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.1545e-04 - val_loss: 4.1076e-04\n",
      "Epoch 98/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.0493e-04 - val_loss: 4.1072e-04\n",
      "Epoch 99/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 2.0197e-04 - val_loss: 4.0763e-04\n",
      "Epoch 100/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.0134e-04 - val_loss: 4.0754e-04\n",
      "Epoch 101/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.9660e-04 - val_loss: 4.0996e-04\n",
      "Epoch 102/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.9053e-04 - val_loss: 4.0953e-04\n",
      "Epoch 103/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.8412e-04 - val_loss: 4.1002e-04\n",
      "Epoch 104/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.8417e-04 - val_loss: 4.0917e-04\n",
      "Epoch 105/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.9210e-04 - val_loss: 4.0559e-04\n",
      "Epoch 106/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.0719e-04 - val_loss: 4.0449e-04\n",
      "Epoch 107/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 2.1473e-04 - val_loss: 4.0381e-04\n",
      "Epoch 108/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 2.2481e-04 - val_loss: 4.0015e-04\n",
      "Epoch 109/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.2423e-04 - val_loss: 4.0476e-04\n",
      "Epoch 110/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.1844e-04 - val_loss: 4.0763e-04\n",
      "Epoch 111/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 2.1225e-04 - val_loss: 4.0442e-04\n",
      "Epoch 112/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.9617e-04 - val_loss: 4.0434e-04\n",
      "Epoch 113/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.8678e-04 - val_loss: 4.1221e-04\n",
      "Epoch 114/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 1.7734e-04 - val_loss: 4.1486e-04\n",
      "Epoch 115/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.7204e-04 - val_loss: 4.0881e-04\n",
      "Epoch 116/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.7161e-04 - val_loss: 4.1165e-04\n",
      "Epoch 117/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.6692e-04 - val_loss: 4.1730e-04\n",
      "Epoch 118/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.6358e-04 - val_loss: 4.1212e-04\n",
      "Epoch 119/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.6083e-04 - val_loss: 4.0982e-04\n",
      "Epoch 120/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.6160e-04 - val_loss: 4.1004e-04\n",
      "Epoch 121/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.6232e-04 - val_loss: 4.1208e-04\n",
      "Epoch 122/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.7384e-04 - val_loss: 4.0997e-04\n",
      "Epoch 123/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.7690e-04 - val_loss: 4.0957e-04\n",
      "Epoch 124/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.7474e-04 - val_loss: 4.0842e-04\n",
      "Epoch 125/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.8503e-04 - val_loss: 4.0810e-04\n",
      "Epoch 126/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.9987e-04 - val_loss: 4.0533e-04\n",
      "Epoch 127/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.8842e-04 - val_loss: 4.0746e-04\n",
      "Epoch 128/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.7452e-04 - val_loss: 4.0620e-04\n",
      "Epoch 129/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.6479e-04 - val_loss: 4.0705e-04\n",
      "Epoch 130/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.5403e-04 - val_loss: 4.0791e-04\n",
      "Epoch 131/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.4717e-04 - val_loss: 4.0938e-04\n",
      "Epoch 132/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.4535e-04 - val_loss: 4.0886e-04\n",
      "Epoch 133/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.4720e-04 - val_loss: 4.0927e-04\n",
      "Epoch 134/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.4733e-04 - val_loss: 4.0936e-04\n",
      "Epoch 135/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.4947e-04 - val_loss: 4.0967e-04\n",
      "Epoch 136/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.4882e-04 - val_loss: 4.1012e-04\n",
      "Epoch 137/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.3933e-04 - val_loss: 4.1074e-04\n",
      "Epoch 138/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 1.3588e-04 - val_loss: 4.1145e-04\n",
      "Epoch 139/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.3456e-04 - val_loss: 4.1089e-04\n",
      "Epoch 140/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.3841e-04 - val_loss: 4.1147e-04\n",
      "Epoch 141/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.4401e-04 - val_loss: 4.1153e-04\n",
      "Epoch 142/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.5170e-04 - val_loss: 4.1391e-04\n",
      "Epoch 143/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.4890e-04 - val_loss: 4.1506e-04\n",
      "Epoch 144/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.4804e-04 - val_loss: 4.1373e-04\n",
      "Epoch 145/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.5370e-04 - val_loss: 4.1379e-04\n",
      "Epoch 146/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.6411e-04 - val_loss: 4.1135e-04\n",
      "Epoch 147/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.8284e-04 - val_loss: 4.1312e-04\n",
      "Epoch 148/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.8092e-04 - val_loss: 4.1352e-04\n",
      "Epoch 149/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.6555e-04 - val_loss: 4.1213e-04\n",
      "Epoch 150/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.4144e-04 - val_loss: 4.1002e-04\n",
      "Epoch 151/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.2405e-04 - val_loss: 4.0933e-04\n",
      "Epoch 152/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.1757e-04 - val_loss: 4.1105e-04\n",
      "Epoch 153/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.1390e-04 - val_loss: 4.0890e-04\n",
      "Epoch 154/300\n",
      "301/301 [==============================] - 151s 502ms/step - loss: 1.1280e-04 - val_loss: 4.1062e-04\n",
      "Epoch 155/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.1178e-04 - val_loss: 4.1523e-04\n",
      "Epoch 156/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.1039e-04 - val_loss: 4.1371e-04\n",
      "Epoch 157/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.1137e-04 - val_loss: 4.1281e-04\n",
      "Epoch 158/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.1294e-04 - val_loss: 4.1156e-04\n",
      "Epoch 159/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0787e-04 - val_loss: 4.1386e-04\n",
      "Epoch 160/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0555e-04 - val_loss: 4.1093e-04\n",
      "Epoch 161/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.1759e-04 - val_loss: 4.1371e-04\n",
      "Epoch 162/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.1806e-04 - val_loss: 4.1277e-04\n",
      "Epoch 163/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.0558e-04 - val_loss: 4.0768e-04\n",
      "Epoch 164/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 9.5548e-05 - val_loss: 4.0500e-04\n",
      "Epoch 165/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 9.4772e-05 - val_loss: 4.0678e-04\n",
      "Epoch 166/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0093e-04 - val_loss: 4.1078e-04\n",
      "Epoch 167/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.0279e-04 - val_loss: 4.1433e-04\n",
      "Epoch 168/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0188e-04 - val_loss: 4.1246e-04\n",
      "Epoch 169/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0828e-04 - val_loss: 4.1083e-04\n",
      "Epoch 170/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 1.1172e-04 - val_loss: 4.1284e-04\n",
      "Epoch 171/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.1600e-04 - val_loss: 4.1645e-04\n",
      "Epoch 172/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.2697e-04 - val_loss: 4.1394e-04\n",
      "Epoch 173/300\n",
      "301/301 [==============================] - 151s 502ms/step - loss: 1.2197e-04 - val_loss: 4.1803e-04\n",
      "Epoch 174/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.0954e-04 - val_loss: 4.1355e-04\n",
      "Epoch 175/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0252e-04 - val_loss: 4.1509e-04\n",
      "Epoch 176/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 9.8585e-05 - val_loss: 4.1639e-04\n",
      "Epoch 177/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 9.7597e-05 - val_loss: 4.1516e-04\n",
      "Epoch 178/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 9.8453e-05 - val_loss: 4.1393e-04\n",
      "Epoch 179/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.0402e-04 - val_loss: 4.1779e-04\n",
      "Epoch 180/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.0789e-04 - val_loss: 4.1917e-04\n",
      "Epoch 181/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.1738e-04 - val_loss: 4.2022e-04\n",
      "Epoch 182/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.2482e-04 - val_loss: 4.1689e-04\n",
      "Epoch 183/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.2304e-04 - val_loss: 4.1779e-04\n",
      "Epoch 184/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.1644e-04 - val_loss: 4.1525e-04\n",
      "Epoch 185/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0773e-04 - val_loss: 4.1750e-04\n",
      "Epoch 186/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0068e-04 - val_loss: 4.1811e-04\n",
      "Epoch 187/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 9.8938e-05 - val_loss: 4.2078e-04\n",
      "Epoch 188/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.0322e-04 - val_loss: 4.2259e-04\n",
      "Epoch 189/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0791e-04 - val_loss: 4.2302e-04\n",
      "Epoch 190/300\n",
      "301/301 [==============================] - 151s 502ms/step - loss: 1.0584e-04 - val_loss: 4.2134e-04\n",
      "Epoch 191/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.0886e-04 - val_loss: 4.2385e-04\n",
      "Epoch 192/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.1586e-04 - val_loss: 4.2849e-04\n",
      "Epoch 193/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0809e-04 - val_loss: 4.2397e-04\n",
      "Epoch 194/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 9.9680e-05 - val_loss: 4.2287e-04\n",
      "Epoch 195/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 9.5212e-05 - val_loss: 4.2412e-04\n",
      "Epoch 196/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 9.7609e-05 - val_loss: 4.2559e-04\n",
      "Epoch 197/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.0372e-04 - val_loss: 4.2376e-04\n",
      "Epoch 198/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.1457e-04 - val_loss: 4.2542e-04\n",
      "Epoch 199/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.2745e-04 - val_loss: 4.2918e-04\n",
      "Epoch 200/300\n",
      "301/301 [==============================] - 151s 502ms/step - loss: 1.3275e-04 - val_loss: 4.2479e-04\n",
      "Epoch 201/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.3293e-04 - val_loss: 4.2503e-04\n",
      "Epoch 202/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.2594e-04 - val_loss: 4.2927e-04\n",
      "Epoch 203/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.2101e-04 - val_loss: 4.2171e-04\n",
      "Epoch 204/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.1886e-04 - val_loss: 4.2058e-04\n",
      "Epoch 205/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 1.1473e-04 - val_loss: 4.2804e-04\n",
      "Epoch 206/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.1082e-04 - val_loss: 4.3438e-04\n",
      "Epoch 207/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 1.0384e-04 - val_loss: 4.3301e-04\n",
      "Epoch 208/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.0037e-04 - val_loss: 4.3019e-04\n",
      "Epoch 209/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 9.7787e-05 - val_loss: 4.2903e-04\n",
      "Epoch 210/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 9.5966e-05 - val_loss: 4.3166e-04\n",
      "Epoch 211/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 9.3001e-05 - val_loss: 4.3137e-04\n",
      "Epoch 212/300\n",
      "301/301 [==============================] - 151s 502ms/step - loss: 9.3959e-05 - val_loss: 4.3157e-04\n",
      "Epoch 213/300\n",
      "301/301 [==============================] - 150s 498ms/step - loss: 1.0099e-04 - val_loss: 4.3034e-04\n",
      "Epoch 214/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.0493e-04 - val_loss: 4.3167e-04\n",
      "Epoch 215/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 150s 499ms/step - loss: 1.0325e-04 - val_loss: 4.2660e-04\n",
      "Epoch 216/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 9.9399e-05 - val_loss: 4.2461e-04\n",
      "Epoch 217/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 9.2745e-05 - val_loss: 4.2219e-04\n",
      "Epoch 218/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.8374e-05 - val_loss: 4.2217e-04\n",
      "Epoch 219/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.6695e-05 - val_loss: 4.2190e-04\n",
      "Epoch 220/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 8.3107e-05 - val_loss: 4.2060e-04\n",
      "Epoch 221/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 7.9975e-05 - val_loss: 4.1959e-04\n",
      "Epoch 222/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.0364e-05 - val_loss: 4.2029e-04\n",
      "Epoch 223/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.0054e-05 - val_loss: 4.2031e-04\n",
      "Epoch 224/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.1419e-05 - val_loss: 4.2002e-04\n",
      "Epoch 225/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.6523e-05 - val_loss: 4.2406e-04\n",
      "Epoch 226/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 8.9231e-05 - val_loss: 4.2521e-04\n",
      "Epoch 227/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 9.3929e-05 - val_loss: 4.2196e-04\n",
      "Epoch 228/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 9.0288e-05 - val_loss: 4.2200e-04\n",
      "Epoch 229/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 9.0919e-05 - val_loss: 4.2171e-04\n",
      "Epoch 230/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 8.7757e-05 - val_loss: 4.1773e-04\n",
      "Epoch 231/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.7900e-05 - val_loss: 4.1370e-04\n",
      "Epoch 232/300\n",
      "301/301 [==============================] - 151s 503ms/step - loss: 8.7952e-05 - val_loss: 4.1584e-04\n",
      "Epoch 233/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 8.4310e-05 - val_loss: 4.2210e-04\n",
      "Epoch 234/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 8.3287e-05 - val_loss: 4.2188e-04\n",
      "Epoch 235/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.5192e-05 - val_loss: 4.2009e-04\n",
      "Epoch 236/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.6456e-05 - val_loss: 4.2279e-04\n",
      "Epoch 237/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.8769e-05 - val_loss: 4.2941e-04\n",
      "Epoch 238/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 9.0678e-05 - val_loss: 4.2799e-04\n",
      "Epoch 239/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 9.5180e-05 - val_loss: 4.2353e-04\n",
      "Epoch 240/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.1178e-04 - val_loss: 4.2210e-04\n",
      "Epoch 241/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 1.2061e-04 - val_loss: 4.1972e-04\n",
      "Epoch 242/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 1.2060e-04 - val_loss: 4.2321e-04\n",
      "Epoch 243/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 1.0277e-04 - val_loss: 4.2096e-04\n",
      "Epoch 244/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.4295e-05 - val_loss: 4.1803e-04\n",
      "Epoch 245/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 7.2285e-05 - val_loss: 4.2059e-04\n",
      "Epoch 246/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 6.5634e-05 - val_loss: 4.1944e-04\n",
      "Epoch 247/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 6.4942e-05 - val_loss: 4.1973e-04\n",
      "Epoch 248/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 6.4309e-05 - val_loss: 4.2414e-04\n",
      "Epoch 249/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 6.1087e-05 - val_loss: 4.2343e-04\n",
      "Epoch 250/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.9434e-05 - val_loss: 4.2257e-04\n",
      "Epoch 251/300\n",
      "301/301 [==============================] - 149s 496ms/step - loss: 5.9473e-05 - val_loss: 4.2576e-04\n",
      "Epoch 252/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 6.1189e-05 - val_loss: 4.2647e-04\n",
      "Epoch 253/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.9109e-05 - val_loss: 4.2342e-04\n",
      "Epoch 254/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.6486e-05 - val_loss: 4.2487e-04\n",
      "Epoch 255/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 5.6493e-05 - val_loss: 4.2574e-04\n",
      "Epoch 256/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 6.0203e-05 - val_loss: 4.3066e-04\n",
      "Epoch 257/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 6.1410e-05 - val_loss: 4.3138e-04\n",
      "Epoch 258/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 6.2071e-05 - val_loss: 4.3399e-04\n",
      "Epoch 259/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 8.0903e-05 - val_loss: 4.3445e-04\n",
      "Epoch 260/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 8.3850e-05 - val_loss: 4.2511e-04\n",
      "Epoch 261/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 6.8479e-05 - val_loss: 4.2532e-04\n",
      "Epoch 262/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.8789e-05 - val_loss: 4.2564e-04\n",
      "Epoch 263/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.6166e-05 - val_loss: 4.2966e-04\n",
      "Epoch 264/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.7151e-05 - val_loss: 4.2766e-04\n",
      "Epoch 265/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.7775e-05 - val_loss: 4.3161e-04\n",
      "Epoch 266/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.7840e-05 - val_loss: 4.3357e-04\n",
      "Epoch 267/300\n",
      "301/301 [==============================] - 150s 498ms/step - loss: 5.7837e-05 - val_loss: 4.3271e-04\n",
      "Epoch 268/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.7023e-05 - val_loss: 4.3344e-04\n",
      "Epoch 269/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.7968e-05 - val_loss: 4.3037e-04\n",
      "Epoch 270/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 6.6404e-05 - val_loss: 4.2634e-04\n",
      "Epoch 271/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 8.1160e-05 - val_loss: 4.3226e-04\n",
      "Epoch 272/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 8.7258e-05 - val_loss: 4.3201e-04\n",
      "Epoch 273/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 7.9054e-05 - val_loss: 4.2694e-04\n",
      "Epoch 274/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 6.9462e-05 - val_loss: 4.2828e-04\n",
      "Epoch 275/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 6.1940e-05 - val_loss: 4.3443e-04\n",
      "Epoch 276/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.8695e-05 - val_loss: 4.3324e-04\n",
      "Epoch 277/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.5900e-05 - val_loss: 4.3118e-04\n",
      "Epoch 278/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.5539e-05 - val_loss: 4.2987e-04\n",
      "Epoch 279/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.4911e-05 - val_loss: 4.3536e-04\n",
      "Epoch 280/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.2088e-05 - val_loss: 4.3154e-04\n",
      "Epoch 281/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.0752e-05 - val_loss: 4.2724e-04\n",
      "Epoch 282/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 5.0527e-05 - val_loss: 4.2906e-04\n",
      "Epoch 283/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.1790e-05 - val_loss: 4.3362e-04\n",
      "Epoch 284/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 5.6325e-05 - val_loss: 4.2926e-04\n",
      "Epoch 285/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 6.3114e-05 - val_loss: 4.3067e-04\n",
      "Epoch 286/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 6.8206e-05 - val_loss: 4.2455e-04\n",
      "Epoch 287/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 7.8242e-05 - val_loss: 4.2751e-04\n",
      "Epoch 288/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 7.5291e-05 - val_loss: 4.3214e-04\n",
      "Epoch 289/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 6.8525e-05 - val_loss: 4.3478e-04\n",
      "Epoch 290/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 7.0162e-05 - val_loss: 4.3617e-04\n",
      "Epoch 291/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 7.4306e-05 - val_loss: 4.3550e-04\n",
      "Epoch 292/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 7.0472e-05 - val_loss: 4.2892e-04\n",
      "Epoch 293/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 6.1931e-05 - val_loss: 4.3195e-04\n",
      "Epoch 294/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.7007e-05 - val_loss: 4.3796e-04\n",
      "Epoch 295/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 5.4790e-05 - val_loss: 4.3565e-04\n",
      "Epoch 296/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 5.6833e-05 - val_loss: 4.3430e-04\n",
      "Epoch 297/300\n",
      "301/301 [==============================] - 151s 500ms/step - loss: 6.2294e-05 - val_loss: 4.4124e-04\n",
      "Epoch 298/300\n",
      "301/301 [==============================] - 150s 499ms/step - loss: 6.3724e-05 - val_loss: 4.4050e-04\n",
      "Epoch 299/300\n",
      "301/301 [==============================] - 151s 501ms/step - loss: 6.6268e-05 - val_loss: 4.3802e-04\n",
      "Epoch 300/300\n",
      "301/301 [==============================] - 150s 500ms/step - loss: 6.8034e-05 - val_loss: 4.4120e-04\n"
     ]
    }
   ],
   "source": [
    "batch = 4\n",
    "epochs = 300\n",
    "encoder_frames=16\n",
    "decoder_frames=16\n",
    "\n",
    "training_generator   = DataGenerator(dataset,batch_size=batch,frame=encoder_frames,mode=\"train\")\n",
    "validation_generator = DataGenerator(val_dataset,batch_size=batch,frame=encoder_frames,mode=\"valid\")\n",
    "\n",
    "history = model.fit_generator(generator=training_generator,validation_data=validation_generator,workers=0,epochs=epochs,use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff999c78cf8>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('RainNet.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_gpu)",
   "language": "python",
   "name": "conda_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
