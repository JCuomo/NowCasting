{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two different ways of importing the models and making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model/params/checkpoint by giving paths: preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import context\n",
    "from mlnowcasting.predict import predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving minimum arguments. It uses default paths and name conventions.\n",
    "\n",
    "Here it's assuming that:\n",
    " - the dataset is in the default location \"mlnowcasting/data/datasets/\"\n",
    " - the checkpoint is in the default location \"mlnowcasting/data/checkpoint/\"\n",
    " - the checkpoint has the same name as the model (ending in .pth)\n",
    " - the parameters file is in the default location \"mlnowcasting/data/checkpoint/\"\n",
    " - the parameters has the same name as the model (ending in .params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint 'convGRU_16_16.pth' (trained for 300 epochs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 16, 64, 64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset    = 'example_dataset'\n",
    "model_name = 'convGRU_16_16'\n",
    "th_dbz = 20 # reflectivity threshold for computing the metrics\n",
    "N = 0 # sample from dataset\n",
    "prediction, target = predict(dataset, model_name)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving all possible arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint 'convGRU_16_16.pth' (trained for 300 epochs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 16, 64, 64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name          = 'convGRU_16_16'\n",
    "dataset             = '../data/datasets/example_dataset.npy'\n",
    "checkpoint_filename = '../data/checkpoints/convGRU_16_16.pth'\n",
    "params_filename     = '../data/checkpoints/convGRU_16_16.param'\n",
    "th_dbz = 20 # reflectivity threshold for computing the metrics\n",
    "N = 0 # sample from dataset\n",
    "prediction, target = predict(dataset, model_name, checkpoint_filename, params_filename, N)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model/params as modules: alternative for developers\n",
    "It assumes that the paramerter file use for the training remains the same, and it contains the path to the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import context\n",
    "import torch\n",
    "import numpy as np\n",
    "from mlnowcasting.models.architectures.convGRU_16_16 import get_model\n",
    "from mlnowcasting.models.params.example_params import params\n",
    "from mlnowcasting.utils.torch_trainer import Torch_Trainer\n",
    "from mlnowcasting.utils.utils import any2reflectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = '../data/datasets/example_dataset.npy'\n",
    "th_dbz = 20 # reflectivity threshold for computing the metrics\n",
    "N = 0 # sample from testing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint 'convGRU_16_16.pth' (trained for 300 epochs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 1, 16, 64, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    T = Torch_Trainer(None, None, get_model, None)\n",
    "    T.load_checkpoint(params)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    test = np.load(testing_dataset)\n",
    "    fi = params['in_frames']\n",
    "    fo = params['out_frames']\n",
    "    context = np.expand_dims(test[:,:fi],1)/255\n",
    "    target  = np.expand_dims(test[:,fi:fi+fo],1)/255\n",
    "    target  = torch.tensor(target).to(device).float()\n",
    "    prediction = T.model(torch.tensor(context).to(device).float())\n",
    "    target  = any2reflectivity(target)\n",
    "    prediction = any2reflectivity(prediction)\n",
    "prediction.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu)",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
